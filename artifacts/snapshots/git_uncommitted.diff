diff --git a/config/litellm.yaml b/config/litellm.yaml
index 038eb76..864e27e 100644
--- a/config/litellm.yaml
+++ b/config/litellm.yaml
@@ -42,6 +42,20 @@ litellm_settings:
   enable_message_redaction: false
   force_ipv4: true
 
+  # Redis caching for LLM responses
+  cache: true
+  cache_params:
+    type: "redis"
+    host: "vllm-studio-redis"
+    port: 6379
+    ttl: 3600  # 1 hour cache
+    namespace: "litellm:cache"
+    supported_call_types: ["acompletion", "completion", "embedding"]
+
+  # Prometheus metrics collection
+  success_callback: ["prometheus"]
+  failure_callback: ["prometheus"]
+
 # General settings
 general_settings:
   master_key: os.environ/LITELLM_MASTER_KEY
@@ -56,9 +70,17 @@ general_settings:
   store_model_in_db: true
 
   # Health and monitoring
-  alerting: null
   health_check_interval: 30
   disable_end_user_cost_tracking: false
 
+  # Alerting configuration (optional Slack integration)
+  alerting: ["slack"]
+  alerting_threshold: 300  # Alert if request takes >5min
+  slack_webhook_url: os.environ/SLACK_WEBHOOK_URL  # Optional: set in .env
+
+  # Performance optimizations
+  batch_spend_updates: true
+  spend_update_interval: 60  # Update spend every 60s
+
   # Connection resilience
   disable_retry_on_max_parallel_request_limit_error: false
diff --git a/controller/app.py b/controller/app.py
index 0d116ef..ca11a39 100644
--- a/controller/app.py
+++ b/controller/app.py
@@ -12,9 +12,14 @@ from typing import Optional
 import httpx
 from fastapi import Depends, FastAPI, HTTPException, Request
 from fastapi.middleware.cors import CORSMiddleware
-from fastapi.responses import JSONResponse
+from fastapi.responses import JSONResponse, StreamingResponse
 
 from . import __version__
+from .events import event_manager, Event
+from .metrics import (
+    update_active_model, update_gpu_metrics, update_sse_metrics,
+    get_metrics_content, get_metrics_content_type
+)
 from .config import settings
 from .gpu import get_gpu_info
 from .models import HealthResponse, LaunchResult, OpenAIModelInfo, OpenAIModelList, Recipe
@@ -38,6 +43,10 @@ app.add_middleware(
 # Global state
 _store: Optional[RecipeStore] = None
 _switch_lock = asyncio.Lock()
+_broadcast_task: Optional[asyncio.Task] = None
+
+import logging
+logger = logging.getLogger(__name__)
 
 
 def get_store() -> RecipeStore:
@@ -63,6 +72,109 @@ async def auth_middleware(request: Request, call_next):
     return await call_next(request)
 
 
+# --- Real-time events (SSE) startup/shutdown ---
+@app.on_event("startup")
+async def startup_event():
+    """Start background tasks on application startup."""
+    global _broadcast_task
+    _broadcast_task = asyncio.create_task(broadcast_updates())
+    logger.info("Started background SSE broadcast task")
+
+
+@app.on_event("shutdown")
+async def shutdown_event():
+    """Clean up background tasks on shutdown."""
+    global _broadcast_task
+    if _broadcast_task:
+        _broadcast_task.cancel()
+        try:
+            await _broadcast_task
+        except asyncio.CancelledError:
+            pass
+    logger.info("Stopped background SSE broadcast task")
+
+
+async def broadcast_updates():
+    """Background task to broadcast status/GPU/metrics updates every second."""
+    while True:
+        try:
+            # Broadcast process status
+            current = find_inference_process(settings.inference_port)
+            status_data = {
+                "running": current is not None,
+                "process": current.model_dump() if current else None,
+                "inference_port": settings.inference_port,
+            }
+            await event_manager.publish_status(status_data)
+
+            # Broadcast GPU metrics
+            gpu_list = get_gpu_info()
+            await event_manager.publish_gpu([gpu.model_dump() for gpu in gpu_list])
+
+            # Broadcast vLLM metrics (if backend is running)
+            if current:
+                try:
+                    async with httpx.AsyncClient(timeout=2) as client:
+                        r = await client.get(f"http://localhost:{settings.inference_port}/metrics")
+                        if r.status_code == 200:
+                            metrics = parse_vllm_metrics(r.text)
+                            await event_manager.publish_metrics(metrics)
+                except Exception:
+                    pass  # Backend not ready or metrics not available
+
+        except Exception as e:
+            logger.error(f"Error in broadcast_updates: {e}")
+
+        await asyncio.sleep(1)  # Update every 1 second
+
+
+def parse_vllm_metrics(prometheus_text: str) -> dict:
+    """Parse Prometheus format metrics from vLLM.
+
+    Extracts key metrics like:
+    - vllm:num_requests_running
+    - vllm:num_requests_waiting
+    - vllm:avg_generation_throughput_toks_per_s
+    - vllm:gpu_cache_usage_perc
+    """
+    metrics = {}
+
+    for line in prometheus_text.split('\n'):
+        if line.startswith('#') or not line.strip():
+            continue
+
+        try:
+            parts = line.split()
+            if len(parts) >= 2:
+                metric_name = parts[0]
+                metric_value = float(parts[1])
+
+                # Map vLLM metrics to friendly names
+                if 'num_requests_running' in metric_name:
+                    metrics['running_requests'] = int(metric_value)
+                elif 'num_requests_waiting' in metric_name:
+                    metrics['pending_requests'] = int(metric_value)
+                elif 'avg_generation_throughput' in metric_name:
+                    metrics['generation_throughput'] = metric_value
+                elif 'avg_prompt_throughput' in metric_name:
+                    metrics['prompt_throughput'] = metric_value
+                elif 'gpu_cache_usage_perc' in metric_name:
+                    metrics['kv_cache_usage'] = metric_value / 100
+                elif 'time_to_first_token' in metric_name:
+                    if 'sum' in metric_name:
+                        metrics['ttft_sum'] = metric_value
+                    elif 'count' in metric_name:
+                        metrics['ttft_count'] = int(metric_value)
+        except Exception:
+            continue
+
+    # Calculate average TTFT if available
+    if 'ttft_sum' in metrics and 'ttft_count' in metrics and metrics['ttft_count'] > 0:
+        metrics['avg_ttft_ms'] = (metrics['ttft_sum'] / metrics['ttft_count']) * 1000
+
+    return metrics
+
+
 # --- Health ---
 @app.get("/health", response_model=HealthResponse, tags=["System"])
 async def health():
@@ -258,13 +370,80 @@ async def delete_recipe(recipe_id: str, store: RecipeStore = Depends(get_store))
 # --- Model lifecycle ---
 @app.post("/launch/{recipe_id}", response_model=LaunchResult, tags=["Lifecycle"])
 async def launch(recipe_id: str, force: bool = False, store: RecipeStore = Depends(get_store)):
-    """Launch a model by recipe ID."""
+    """Launch a model by recipe ID with real-time progress updates.
+
+    Progress events are emitted via SSE:
+    - evicting: Stopping current model
+    - launching: Starting new model
+    - waiting: Waiting for model to be ready
+    - ready: Model is ready to serve
+    - error: Launch failed
+    """
+    import time
+
     recipe = store.get(recipe_id)
     if not recipe:
         raise HTTPException(status_code=404, detail="Recipe not found")
 
     async with _switch_lock:
-        success, pid, message = await switch_model(recipe, force=force)
+        # Stage 1: Evict current model
+        await event_manager.publish_launch_progress(
+            recipe_id, "evicting", "Stopping current model...", progress=0.0
+        )
+        await evict_model(force=force)
+        await asyncio.sleep(2)
+
+        # Stage 2: Launch new model
+        await event_manager.publish_launch_progress(
+            recipe_id, "launching", f"Starting {recipe.name}...", progress=0.25
+        )
+
+        # Import launch_model from process module
+        from .process import launch_model
+        success, pid, message = await launch_model(recipe)
+
+        if not success:
+            await event_manager.publish_launch_progress(
+                recipe_id, "error", message, progress=0.0
+            )
+            return LaunchResult(success=False, pid=None, message=message, log_file=None)
+
+        # Stage 3: Wait for readiness
+        await event_manager.publish_launch_progress(
+            recipe_id, "waiting", "Waiting for model to load...", progress=0.5
+        )
+
+        # Poll health endpoint (up to 5 minutes)
+        start = time.time()
+        timeout = 300
+        ready = False
+
+        while time.time() - start < timeout:
+            try:
+                async with httpx.AsyncClient(timeout=5) as client:
+                    r = await client.get(f"http://localhost:{settings.inference_port}/health")
+                    if r.status_code == 200:
+                        ready = True
+                        break
+            except Exception:
+                pass
+
+            elapsed = int(time.time() - start)
+            await event_manager.publish_launch_progress(
+                recipe_id, "waiting",
+                f"Loading model... ({elapsed}s)",
+                progress=0.5 + (elapsed / timeout) * 0.5
+            )
+            await asyncio.sleep(3)
+
+        if ready:
+            await event_manager.publish_launch_progress(
+                recipe_id, "ready", "Model is ready!", progress=1.0
+            )
+        else:
+            await event_manager.publish_launch_progress(
+                recipe_id, "error", "Model failed to become ready (timeout)", progress=0.0
+            )
 
     return LaunchResult(
         success=success,
@@ -372,6 +551,114 @@ async def delete_logs(session_id: str):
     return {"success": True}
 
 
+# --- Real-time events (SSE) endpoints ---
+@app.get("/events", tags=["Events"])
+async def events_stream():
+    """Subscribe to real-time status updates via Server-Sent Events.
+
+    Events emitted:
+    - status: Process status (running/stopped, model info)
+    - gpu: GPU metrics (utilization, memory, temperature)
+    - metrics: vLLM performance metrics
+    - launch_progress: Model launch progress
+
+    Example usage:
+        const es = new EventSource('/events');
+        es.addEventListener('status', (e) => console.log(JSON.parse(e.data)));
+    """
+    async def event_generator():
+        try:
+            async for event in event_manager.subscribe():
+                yield event.to_sse()
+        except asyncio.CancelledError:
+            pass  # Client disconnected
+
+    return StreamingResponse(
+        event_generator(),
+        media_type="text/event-stream",
+        headers={
+            "Cache-Control": "no-cache, no-transform",
+            "Connection": "keep-alive",
+            "X-Accel-Buffering": "no",  # Disable nginx buffering
+        }
+    )
+
+
+@app.get("/logs/{session_id}/stream", tags=["Logs"])
+async def stream_logs(session_id: str):
+    """Stream log file updates in real-time via SSE.
+
+    First sends existing log content, then tails new lines as they're added.
+    """
+    path = _log_path_for(session_id)
+
+    async def log_generator():
+        try:
+            # Send existing content first
+            if path.exists():
+                with path.open("r", encoding="utf-8", errors="replace") as f:
+                    for line in f:
+                        event = Event(type="log", data={"line": line.rstrip("\n")})
+                        yield event.to_sse()
+
+            # Then stream new lines
+            async for event in event_manager.subscribe(f"logs:{session_id}"):
+                yield event.to_sse()
+        except asyncio.CancelledError:
+            pass
+
+    return StreamingResponse(
+        log_generator(),
+        media_type="text/event-stream",
+        headers={
+            "Cache-Control": "no-cache, no-transform",
+            "Connection": "keep-alive",
+        }
+    )
+
+
+@app.get("/events/stats", tags=["Events"])
+async def events_stats():
+    """Get event manager statistics for monitoring."""
+    return event_manager.get_stats()
+
+
+# --- Prometheus metrics ---
+@app.get("/metrics", tags=["Monitoring"])
+async def metrics():
+    """Prometheus metrics endpoint.
+
+    Exposes metrics for:
+    - Model switches and launch failures
+    - GPU utilization, memory, temperature
+    - SSE connection stats
+    - Active model information
+    """
+    from fastapi.responses import Response
+
+    # Update metrics before serving
+    current = find_inference_process(settings.inference_port)
+    if current:
+        update_active_model(
+            model_path=current.model_path,
+            backend=current.backend,
+            served_name=current.served_model_name
+        )
+    else:
+        update_active_model()
+
+    gpu_list = get_gpu_info()
+    update_gpu_metrics([gpu.model_dump() for gpu in gpu_list])
+
+    sse_stats = event_manager.get_stats()
+    update_sse_metrics(sse_stats)
+
+    return Response(
+        content=get_metrics_content(),
+        media_type=get_metrics_content_type()
+    )
+
+
 # --- MCP (minimal built-in tools) ---
 _MCP_CFG_NAME = "mcp_servers.json"
 
diff --git a/controller/backends.py b/controller/backends.py
index 1719cf2..d1a15e2 100644
--- a/controller/backends.py
+++ b/controller/backends.py
@@ -3,16 +3,51 @@
 from __future__ import annotations
 
 import json
-from typing import List
+import os
+from typing import Any, Dict, List, Optional
 
 from .config import settings
 from .models import Recipe
 
 
+def _get_extra_arg(extra_args: Dict[str, Any], key: str) -> Any:
+    """Get extra_args value accepting both snake_case and kebab-case keys."""
+    if key in extra_args:
+        return extra_args[key]
+    kebab = key.replace("_", "-")
+    if kebab in extra_args:
+        return extra_args[kebab]
+    snake = key.replace("-", "_")
+    if snake in extra_args:
+        return extra_args[snake]
+    return None
+
+
+def _get_python_path(recipe: Recipe) -> Optional[str]:
+    """Get Python path from recipe.python_path or extra_args.venv_path."""
+    # Explicit python_path takes priority
+    if recipe.python_path:
+        return recipe.python_path
+
+    # Check for venv_path in extra_args
+    venv_path = _get_extra_arg(recipe.extra_args, "venv_path")
+    if venv_path:
+        python_bin = os.path.join(venv_path, "bin", "python")
+        if os.path.exists(python_bin):
+            return python_bin
+
+    return None
+
+
 def build_vllm_command(recipe: Recipe) -> List[str]:
     """Build vLLM launch command."""
-    if recipe.python_path:
-        cmd = [recipe.python_path, "-m", "vllm.entrypoints.openai.api_server"]
+    python_path = _get_python_path(recipe)
+    if python_path:
+        vllm_bin = os.path.join(os.path.dirname(python_path), "vllm")
+        if os.path.exists(vllm_bin):
+            cmd = [vllm_bin, "serve"]
+        else:
+            cmd = [python_path, "-m", "vllm.entrypoints.openai.api_server"]
     else:
         cmd = ["vllm", "serve"]
 
@@ -46,7 +81,7 @@ def build_vllm_command(recipe: Recipe) -> List[str]:
 
 def build_sglang_command(recipe: Recipe) -> List[str]:
     """Build SGLang launch command."""
-    python = recipe.python_path or settings.sglang_python or "python"
+    python = _get_python_path(recipe) or settings.sglang_python or "python"
     cmd = [python, "-m", "sglang.launch_server"]
     cmd.extend(["--model-path", recipe.model_path])
     cmd.extend(["--host", recipe.host, "--port", str(recipe.port)])
@@ -70,8 +105,16 @@ def build_sglang_command(recipe: Recipe) -> List[str]:
 
 def _append_extra_args(cmd: List[str], extra_args: dict) -> None:
     """Append extra CLI arguments to command."""
+    # Keys that are used by the controller, not passed to the backend
+    INTERNAL_KEYS = {"venv_path", "env_vars", "cuda_visible_devices", "description", "tags"}
+
     for key, value in extra_args.items():
+        normalized_key = key.replace("-", "_").lower()
+        if normalized_key in INTERNAL_KEYS:
+            continue
         flag = f"--{key.replace('_', '-')}"
+        if flag in cmd:
+            continue
         if value is True:
             cmd.append(flag)
         elif value not in (False, None):
diff --git a/controller/process.py b/controller/process.py
index 6439b29..732e5a3 100644
--- a/controller/process.py
+++ b/controller/process.py
@@ -3,14 +3,14 @@
 from __future__ import annotations
 
 import asyncio
-import json
 import os
 import subprocess
 from pathlib import Path
-from typing import List, Optional, Tuple
+from typing import Dict, List, Optional, Tuple
 
 import psutil
 
+from .backends import build_sglang_command, build_vllm_command
 from .config import settings
 from .models import Backend, ProcessInfo, Recipe
 
@@ -23,6 +23,31 @@ def _extract_flag(cmdline: List[str], flag: str) -> Optional[str]:
     return None
 
 
+def _build_env(recipe: Recipe) -> Dict[str, str]:
+    env = os.environ.copy()
+
+    env_vars = (
+        recipe.extra_args.get("env_vars")
+        or recipe.extra_args.get("env-vars")
+        or recipe.extra_args.get("envVars")
+    )
+    if isinstance(env_vars, dict):
+        for k, v in env_vars.items():
+            if v is None:
+                continue
+            env[str(k)] = str(v)
+
+    cuda_visible_devices = (
+        recipe.extra_args.get("cuda_visible_devices")
+        or recipe.extra_args.get("cuda-visible-devices")
+        or recipe.extra_args.get("CUDA_VISIBLE_DEVICES")
+    )
+    if cuda_visible_devices not in (None, "", False):
+        env["CUDA_VISIBLE_DEVICES"] = str(cuda_visible_devices)
+
+    return env
+
+
 def _is_inference_process(cmdline: List[str]) -> Optional[str]:
     """Check if cmdline is vLLM or SGLang, return backend name."""
     if not cmdline:
@@ -92,81 +117,6 @@ async def kill_process(pid: int, force: bool = False) -> bool:
     return True
 
 
-def build_vllm_command(recipe: Recipe) -> List[str]:
-    """Build vLLM launch command."""
-    if recipe.python_path:
-        cmd = [recipe.python_path, "-m", "vllm.entrypoints.openai.api_server"]
-    else:
-        cmd = ["vllm", "serve"]
-
-    cmd.extend([recipe.model_path, "--host", recipe.host, "--port", str(recipe.port)])
-
-    if recipe.served_model_name:
-        cmd.extend(["--served-model-name", recipe.served_model_name])
-    if recipe.tensor_parallel_size > 1:
-        cmd.extend(["--tensor-parallel-size", str(recipe.tensor_parallel_size)])
-    if recipe.pipeline_parallel_size > 1:
-        cmd.extend(["--pipeline-parallel-size", str(recipe.pipeline_parallel_size)])
-
-    cmd.extend(["--max-model-len", str(recipe.max_model_len)])
-    cmd.extend(["--gpu-memory-utilization", str(recipe.gpu_memory_utilization)])
-    cmd.extend(["--max-num-seqs", str(recipe.max_num_seqs)])
-
-    if recipe.kv_cache_dtype != "auto":
-        cmd.extend(["--kv-cache-dtype", recipe.kv_cache_dtype])
-    if recipe.trust_remote_code:
-        cmd.append("--trust-remote-code")
-    if recipe.tool_call_parser:
-        cmd.extend(["--tool-call-parser", recipe.tool_call_parser, "--enable-auto-tool-choice"])
-    if recipe.quantization:
-        cmd.extend(["--quantization", recipe.quantization])
-    if recipe.dtype:
-        cmd.extend(["--dtype", recipe.dtype])
-
-    # Extra args
-    for key, value in recipe.extra_args.items():
-        flag = f"--{key.replace('_', '-')}"
-        if value is True:
-            cmd.append(flag)
-        elif value not in (False, None):
-            if isinstance(value, (dict, list)):
-                cmd.extend([flag, json.dumps(value)])
-            else:
-                cmd.extend([flag, str(value)])
-
-    return cmd
-
-
-def build_sglang_command(recipe: Recipe) -> List[str]:
-    """Build SGLang launch command."""
-    python = recipe.python_path or settings.sglang_python or "python"
-    cmd = [python, "-m", "sglang.launch_server"]
-    cmd.extend(["--model-path", recipe.model_path])
-    cmd.extend(["--host", recipe.host, "--port", str(recipe.port)])
-
-    if recipe.served_model_name:
-        cmd.extend(["--served-model-name", recipe.served_model_name])
-    if recipe.tensor_parallel_size > 1:
-        cmd.extend(["--tp", str(recipe.tensor_parallel_size)])
-
-    cmd.extend(["--context-length", str(recipe.max_model_len)])
-    cmd.extend(["--mem-fraction-static", str(recipe.gpu_memory_utilization)])
-
-    if recipe.trust_remote_code:
-        cmd.append("--trust-remote-code")
-    if recipe.quantization:
-        cmd.extend(["--quantization", recipe.quantization])
-
-    for key, value in recipe.extra_args.items():
-        flag = f"--{key.replace('_', '-')}"
-        if value is True:
-            cmd.append(flag)
-        elif value not in (False, None):
-            cmd.extend([flag, str(value)])
-
-    return cmd
-
-
 async def launch_model(recipe: Recipe) -> Tuple[bool, Optional[int], str]:
     """Launch inference server with recipe config."""
     recipe.port = settings.inference_port  # Override with configured port
@@ -177,7 +127,7 @@ async def launch_model(recipe: Recipe) -> Tuple[bool, Optional[int], str]:
         cmd = build_vllm_command(recipe)
 
     log_file = Path(f"/tmp/vllm_{recipe.id}.log")
-    env = os.environ.copy()
+    env = _build_env(recipe)
 
     try:
         with open(log_file, "w") as log:
diff --git a/docker-compose.yml b/docker-compose.yml
index bea044a..093a27b 100644
--- a/docker-compose.yml
+++ b/docker-compose.yml
@@ -36,18 +36,74 @@ services:
       timeout: 10s
       retries: 3
 
-  # Frontend (optional)
+  # Redis - Caching and rate limiting for LiteLLM
+  redis:
+    image: redis:7-alpine
+    container_name: vllm-studio-redis
+    ports:
+      - "6379:6379"
+    volumes:
+      - ./data/redis:/data
+    command: redis-server --appendonly yes --save 60 1
+    restart: unless-stopped
+    healthcheck:
+      test: ["CMD", "redis-cli", "ping"]
+      interval: 10s
+      timeout: 5s
+      retries: 3
+
+  # Prometheus - Time-series metrics collection
+  prometheus:
+    image: prom/prometheus:latest
+    container_name: vllm-studio-prometheus
+    ports:
+      - "9090:9090"
+    volumes:
+      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml:ro
+      - ./data/prometheus:/prometheus
+    command:
+      - '--config.file=/etc/prometheus/prometheus.yml'
+      - '--storage.tsdb.path=/prometheus'
+      - '--storage.tsdb.retention.time=30d'
+      - '--web.enable-lifecycle'
+    restart: unless-stopped
+    extra_hosts:
+      - "host.docker.internal:host-gateway"
+
+  # Grafana - Dashboards and visualization
+  grafana:
+    image: grafana/grafana:latest
+    container_name: vllm-studio-grafana
+    ports:
+      - "3001:3000"
+    volumes:
+      - ./config/grafana/provisioning:/etc/grafana/provisioning:ro
+      - ./config/grafana/dashboards:/var/lib/grafana/dashboards:ro
+      - ./data/grafana:/var/lib/grafana
+    environment:
+      - GF_SECURITY_ADMIN_USER=admin
+      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
+      - GF_USERS_ALLOW_SIGN_UP=false
+      - GF_SERVER_ROOT_URL=http://localhost:3001
+      - GF_INSTALL_PLUGINS=redis-datasource
+    restart: unless-stopped
+    depends_on:
+      - prometheus
+      - redis
+
+  # Frontend (uses host network to reach controller on localhost:8080)
   frontend:
+    image: ghcr.io/0xsero/vllmstudio/frontend:latest
     build:
       context: ./frontend
       args:
-        NEXT_PUBLIC_API_URL: ${NEXT_PUBLIC_API_URL:-http://localhost:8080}
+        NEXT_PUBLIC_API_URL: ""
     container_name: vllm-studio-frontend
-    ports:
-      - "3000:3000"
+    network_mode: host
     environment:
-      - NEXT_PUBLIC_API_URL=${NEXT_PUBLIC_API_URL:-http://localhost:8080}
-      - NEXT_PUBLIC_LITELLM_URL=${NEXT_PUBLIC_LITELLM_URL:-http://localhost:4100}
-    extra_hosts:
-      - "host.docker.internal:host-gateway"
+      # Backend URL for server-side API routes (localhost via host network)
+      - BACKEND_URL=http://localhost:8080
+      - API_KEY=${VLLM_STUDIO_API_KEY:-}
+      # LiteLLM for chat (client-side, via tunnel)
+      - NEXT_PUBLIC_LITELLM_URL=${NEXT_PUBLIC_LITELLM_URL:-https://homelabai.org}
     restart: unless-stopped
diff --git a/frontend/src/app/page.tsx b/frontend/src/app/page.tsx
index 7671f35..8f63cec 100644
--- a/frontend/src/app/page.tsx
+++ b/frontend/src/app/page.tsx
@@ -1,17 +1,27 @@
 'use client';
 
 import { useState, useEffect, useCallback } from 'react';
-import { Search, Play, Cpu, Zap, HardDrive, Activity, Clock, Hash, Thermometer, MemoryStick, Settings, MessageSquare, FileText, Square } from 'lucide-react';
+import { Search, Play, Cpu, Zap, HardDrive, Activity, Clock, Hash, Thermometer, MemoryStick, Settings, MessageSquare, FileText, Square, X, Check, Wifi, WifiOff } from 'lucide-react';
 import { useRouter } from 'next/navigation';
 import api from '@/lib/api';
+import { useRealtimeStatus } from '@/hooks/useRealtimeStatus';
 import type { GPU, RecipeWithStatus, ProcessInfo, Metrics } from '@/lib/types';
 
 export default function Dashboard() {
-  const [gpus, setGpus] = useState<GPU[]>([]);
+  // Real-time data from SSE (updates every 1 second)
+  const {
+    status: realtimeStatus,
+    gpus: realtimeGpus,
+    metrics: realtimeMetrics,
+    launchProgress,
+    isConnected,
+    error: connectionError,
+    reconnectAttempts,
+  } = useRealtimeStatus();
+
+  // State for recipes and UI
   const [recipes, setRecipes] = useState<RecipeWithStatus[]>([]);
-  const [currentProcess, setCurrentProcess] = useState<ProcessInfo | null>(null);
   const [currentRecipe, setCurrentRecipe] = useState<RecipeWithStatus | null>(null);
-  const [metrics, setMetrics] = useState<Metrics | null>(null);
   const [logs, setLogs] = useState<string[]>([]);
   const [searchQuery, setSearchQuery] = useState('');
   const [searchResults, setSearchResults] = useState<RecipeWithStatus[]>([]);
@@ -19,22 +29,20 @@ export default function Dashboard() {
   const [launching, setLaunching] = useState(false);
   const router = useRouter();
 
-  const loadData = useCallback(async () => {
+  // Derived state from real-time data
+  const gpus = realtimeGpus.length > 0 ? realtimeGpus : [];
+  const currentProcess = realtimeStatus?.process || null;
+  const metrics = realtimeMetrics;
+
+  // Load recipes (only needs to be done once and when actions occur)
+  const loadRecipes = useCallback(async () => {
     try {
-      const [gpuData, recipesData, statusData, metricsData] = await Promise.all([
-        api.getGPUs(),
-        api.getRecipes(),
-        api.getStatus(),
-        api.getMetrics().catch(() => null),
-      ]);
-
-      setGpus(gpuData.gpus || []);
+      const recipesData = await api.getRecipes();
       const recipesList = recipesData.recipes || [];
       setRecipes(recipesList);
-      setCurrentProcess(statusData.process);
 
-      if (statusData.process) {
-        // Find running recipe from recipes list (which has status)
+      // Find running recipe if there's a current process
+      if (currentProcess) {
         const runningRecipe = recipesList.find((r: RecipeWithStatus) => r.status === 'running');
         setCurrentRecipe(runningRecipe || null);
 
@@ -46,20 +54,24 @@ export default function Dashboard() {
         setCurrentRecipe(null);
         setLogs([]);
       }
-
-      setMetrics(metricsData);
     } catch (e) {
-      console.error('Failed to load data:', e);
+      console.error('Failed to load recipes:', e);
     } finally {
       setLoading(false);
     }
-  }, []);
+  }, [currentProcess]);
 
+  // Initial load and reload when process status changes
   useEffect(() => {
-    loadData();
-    const interval = setInterval(loadData, 5000);
-    return () => clearInterval(interval);
-  }, [loadData]);
+    loadRecipes();
+  }, [loadRecipes]);
+
+  // Refresh recipes when launch progress completes
+  useEffect(() => {
+    if (launchProgress?.stage === 'ready' || launchProgress?.stage === 'error') {
+      loadRecipes();
+    }
+  }, [launchProgress?.stage, loadRecipes]);
 
   useEffect(() => {
     if (searchQuery.trim()) {
@@ -78,9 +90,11 @@ export default function Dashboard() {
   const handleLaunch = async (recipeId: string) => {
     setLaunching(true);
     try {
+      // Launch is now async with progress events via SSE
       await api.switchModel(recipeId, true);
       setSearchQuery('');
-      await loadData();
+      // Progress will be shown via launchProgress from SSE
+      // loadRecipes will be called when progress reaches 'ready' or 'error'
     } catch (e) {
       alert('Failed to launch: ' + (e as Error).message);
     } finally {
@@ -92,7 +106,7 @@ export default function Dashboard() {
     if (!confirm('Stop the current model?')) return;
     try {
       await api.evictModel(true);
-      await loadData();
+      await loadRecipes();
     } catch (e) {
       alert('Failed to stop model: ' + (e as Error).message);
     }
@@ -129,6 +143,18 @@ export default function Dashboard() {
     return num.toFixed(decimals);
   };
 
+  // Convert memory to GB - handles bytes, MB, or already GB
+  const toGB = (value: number): number => {
+    // Bytes: typically > 1 billion for GPU memory (e.g., 25769803776 for 24GB)
+    if (value > 1e10) return value / (1024 * 1024 * 1024);
+    // Also bytes but smaller (e.g., 534773760 for 0.5GB used)
+    if (value > 1e8) return value / (1024 * 1024 * 1024);
+    // MB: typically thousands (e.g., 24576 for 24GB)
+    if (value > 1000) return value / 1024;
+    // Already GB
+    return value;
+  };
+
   if (loading) {
     return (
       <div className="flex items-center justify-center h-[calc(100vh-4rem)]">
@@ -140,27 +166,52 @@ export default function Dashboard() {
   }
 
   return (
-    <div className="p-6 max-w-7xl mx-auto space-y-6 overflow-x-hidden">
+    <div className="p-4 sm:p-6 max-w-6xl mx-auto space-y-4 sm:space-y-6 overflow-x-hidden w-full">
+      {/* Connection status indicator */}
+      {!isConnected && (
+        <div className="fixed top-16 right-4 z-50 px-3 py-2 bg-yellow-500/10 border border-yellow-500/20 rounded-lg shadow-lg animate-pulse">
+          <div className="flex items-center gap-2 text-xs text-yellow-400">
+            <WifiOff className="h-3 w-3" />
+            <span>Reconnecting... (attempt {reconnectAttempts})</span>
+          </div>
+        </div>
+      )}
+
+      {connectionError && isConnected === false && reconnectAttempts >= 10 && (
+        <div className="fixed top-16 right-4 z-50 px-3 py-2 bg-red-500/10 border border-red-500/20 rounded-lg shadow-lg">
+          <div className="flex items-center gap-2 text-xs text-red-400">
+            <X className="h-3 w-3" />
+            <span>Connection failed - updates paused</span>
+          </div>
+        </div>
+      )}
+
       {/* GPU Grid */}
       <section>
-        <h2 className="text-xs font-medium text-[var(--muted-foreground)] uppercase tracking-wide mb-3">GPU Status</h2>
-        <div className="grid grid-cols-2 sm:grid-cols-4 lg:grid-cols-8 gap-3">
+        <h2 className="text-xs font-medium text-[var(--muted-foreground)] uppercase tracking-wide mb-2 sm:mb-3">GPU Status</h2>
+        <div className="grid grid-cols-2 sm:grid-cols-4 md:grid-cols-4 lg:grid-cols-8 gap-2 sm:gap-3">
           {gpus.map((gpu) => {
-            const memPct = Math.round((gpu.memory_used_mb || 0) / (gpu.memory_total_mb || 1) * 100);
+            const memUsedRaw = gpu.memory_used_mb ?? gpu.memory_used ?? 0;
+            const memTotalRaw = gpu.memory_total_mb ?? gpu.memory_total ?? 1;
+            const memUsedGB = toGB(memUsedRaw);
+            const memTotalGB = toGB(memTotalRaw);
+            const memPct = Math.round((memUsedGB / memTotalGB) * 100);
+            const temp = gpu.temp_c ?? gpu.temperature ?? 0;
+            const util = gpu.utilization_pct ?? gpu.utilization ?? 0;
             return (
-              <div key={gpu.id} className="bg-[var(--card)] border border-[var(--border)] rounded-lg p-3">
-                <div className="flex items-center justify-between mb-2">
-                  <span className="text-xs font-medium text-[var(--muted-foreground)]">GPU {gpu.id}</span>
-                  <span className={`text-xs font-mono ${getTempColor(gpu.temp_c || 0)}`}>
-                    {gpu.temp_c || 0}°C
+              <div key={gpu.id ?? gpu.index} className="bg-[var(--card)] border border-[var(--border)] rounded-lg p-2 sm:p-3">
+                <div className="flex items-center justify-between mb-1 sm:mb-2">
+                  <span className="text-[10px] sm:text-xs font-medium text-[var(--muted-foreground)]">GPU {gpu.id ?? gpu.index}</span>
+                  <span className={`text-[10px] sm:text-xs font-mono ${getTempColor(temp)}`}>
+                    {temp}°C
                   </span>
                 </div>
-                <div className="text-[10px] text-[var(--muted)] truncate mb-2">{gpu.name?.replace('NVIDIA GeForce ', '')}</div>
-                <div className="flex items-center justify-between text-xs">
+                <div className="text-[9px] sm:text-[10px] text-[var(--muted)] truncate mb-1 sm:mb-2">{gpu.name?.replace('NVIDIA GeForce ', '').replace('RTX ', '')}</div>
+                <div className="flex items-center justify-between text-[10px] sm:text-xs">
                   <span className={`font-mono ${getMemColor(memPct)}`}>
-                    {((gpu.memory_used_mb || 0) / 1024).toFixed(1)}/{Math.round((gpu.memory_total_mb || 0) / 1024)}G
+                    {memUsedGB.toFixed(1)}/{memTotalGB.toFixed(0)}G
                   </span>
-                  <span className="text-[var(--muted-foreground)]">{gpu.utilization_pct || 0}%</span>
+                  <span className="text-[var(--muted-foreground)]">{util}%</span>
                 </div>
               </div>
             );
@@ -169,14 +220,14 @@ export default function Dashboard() {
       </section>
 
       {/* Main Grid */}
-      <div className="grid lg:grid-cols-3 gap-6">
+      <div className="grid lg:grid-cols-3 gap-4 sm:gap-6">
         {/* Left Column */}
-        <div className="lg:col-span-2 space-y-6">
+        <div className="lg:col-span-2 space-y-4 sm:space-y-6 min-w-0">
           {/* Running Model */}
-          <section className="bg-[var(--card)] border border-[var(--border)] rounded-lg p-6">
-            <div className="flex items-center justify-between mb-4">
+          <section className="bg-[var(--card)] border border-[var(--border)] rounded-lg p-4 sm:p-6">
+            <div className="flex flex-col sm:flex-row sm:items-center justify-between gap-2 sm:gap-0 mb-4">
               <h2 className="text-xs font-medium text-[var(--muted-foreground)] uppercase tracking-wide">Running Model</h2>
-              <div className="flex items-center gap-2">
+              <div className="flex items-center gap-1 sm:gap-2 flex-wrap">
                 {currentProcess ? (
                   <>
                     <button
@@ -184,14 +235,14 @@ export default function Dashboard() {
                       className="flex items-center gap-1 px-2 py-1 border border-[var(--border)] rounded text-xs hover:bg-[var(--card-hover)]"
                       title="Open chat"
                     >
-                      <MessageSquare className="h-3.5 w-3.5" /> Chat
+                      <MessageSquare className="h-3.5 w-3.5" /> <span className="hidden sm:inline">Chat</span>
                     </button>
                     <button
                       onClick={() => router.push('/logs')}
                       className="flex items-center gap-1 px-2 py-1 border border-[var(--border)] rounded text-xs hover:bg-[var(--card-hover)]"
                       title="Open logs"
                     >
-                      <FileText className="h-3.5 w-3.5" /> Logs
+                      <FileText className="h-3.5 w-3.5" /> <span className="hidden sm:inline">Logs</span>
                     </button>
                     {currentRecipe?.id ? (
                       <button
@@ -199,7 +250,7 @@ export default function Dashboard() {
                         className="flex items-center gap-1 px-2 py-1 border border-[var(--border)] rounded text-xs hover:bg-[var(--card-hover)]"
                         title="Edit recipe"
                       >
-                        <Settings className="h-3.5 w-3.5" /> Edit
+                        <Settings className="h-3.5 w-3.5" /> <span className="hidden sm:inline">Edit</span>
                       </button>
                     ) : null}
                     <button
@@ -207,7 +258,7 @@ export default function Dashboard() {
                       className="flex items-center gap-1 px-2 py-1 border border-[var(--border)] rounded text-xs hover:bg-[var(--card-hover)] text-[var(--error)]"
                       title="Stop model"
                     >
-                      <Square className="h-3.5 w-3.5" /> Stop
+                      <Square className="h-3.5 w-3.5" /> <span className="hidden sm:inline">Stop</span>
                     </button>
                   </>
                 ) : null}
@@ -294,8 +345,8 @@ export default function Dashboard() {
           </section>
 
           {/* Quick Launch */}
-          <section className="bg-[var(--card)] border border-[var(--border)] rounded-lg p-6">
-            <h2 className="text-xs font-medium text-[var(--muted-foreground)] uppercase tracking-wide mb-3">Quick Launch</h2>
+          <section className="bg-[var(--card)] border border-[var(--border)] rounded-lg p-4 sm:p-6">
+            <h2 className="text-xs font-medium text-[var(--muted-foreground)] uppercase tracking-wide mb-2 sm:mb-3">Quick Launch</h2>
             <div className="relative">
               <Search className="absolute left-3 top-1/2 -translate-y-1/2 h-4 w-4 text-[var(--muted)]" />
               <input
@@ -339,9 +390,9 @@ export default function Dashboard() {
           </section>
 
           {/* Live Logs */}
-          <section className="bg-[var(--card)] border border-[var(--border)] rounded-lg p-6">
-            <h2 className="text-xs font-medium text-[var(--muted-foreground)] uppercase tracking-wide mb-3">Live Logs</h2>
-            <div className="bg-[var(--background)] rounded-lg p-3 h-64 overflow-auto font-mono text-xs">
+          <section className="bg-[var(--card)] border border-[var(--border)] rounded-lg p-4 sm:p-6">
+            <h2 className="text-xs font-medium text-[var(--muted-foreground)] uppercase tracking-wide mb-2 sm:mb-3">Live Logs</h2>
+            <div className="bg-[var(--background)] rounded-lg p-2 sm:p-3 h-48 sm:h-64 overflow-auto font-mono text-[10px] sm:text-xs">
               {logs.length > 0 ? (
                 logs.map((line, i) => (
                   <div key={i} className={`whitespace-pre-wrap break-all py-0.5 ${
@@ -359,43 +410,43 @@ export default function Dashboard() {
         </div>
 
         {/* Right Column */}
-        <div className="space-y-6">
-          <section className="bg-[var(--card)] border border-[var(--border)] rounded-lg p-6">
-            <h2 className="text-xs font-medium text-[var(--muted-foreground)] uppercase tracking-wide mb-3">Analytics</h2>
-            <div className="space-y-3">
-              <div className="flex items-center justify-between p-3 bg-[var(--background)] rounded-lg">
+        <div className="space-y-4 sm:space-y-6 min-w-0">
+          <section className="bg-[var(--card)] border border-[var(--border)] rounded-lg p-4 sm:p-6">
+            <h2 className="text-xs font-medium text-[var(--muted-foreground)] uppercase tracking-wide mb-2 sm:mb-3">Analytics</h2>
+            <div className="space-y-2 sm:space-y-3">
+              <div className="flex items-center justify-between p-2 sm:p-3 bg-[var(--background)] rounded-lg">
                 <div className="flex items-center gap-2">
-                  <Hash className="h-4 w-4 text-[var(--muted)]" />
-                  <span className="text-sm">Total Requests</span>
+                  <Hash className="h-3 sm:h-4 w-3 sm:w-4 text-[var(--muted)]" />
+                  <span className="text-xs sm:text-sm">Requests</span>
                 </div>
-                <span className="font-mono font-semibold">{metrics?.request_success || 0}</span>
+                <span className="font-mono font-semibold text-sm">{metrics?.request_success || 0}</span>
               </div>
-              <div className="flex items-center justify-between p-3 bg-[var(--background)] rounded-lg">
+              <div className="flex items-center justify-between p-2 sm:p-3 bg-[var(--background)] rounded-lg">
                 <div className="flex items-center gap-2">
-                  <Activity className="h-4 w-4 text-[var(--muted)]" />
-                  <span className="text-sm">Tokens Generated</span>
+                  <Activity className="h-3 sm:h-4 w-3 sm:w-4 text-[var(--muted)]" />
+                  <span className="text-xs sm:text-sm">Tokens</span>
                 </div>
-                <span className="font-mono font-semibold">{metrics?.generation_tokens_total?.toLocaleString() || 0}</span>
+                <span className="font-mono font-semibold text-sm">{metrics?.generation_tokens_total?.toLocaleString() || 0}</span>
               </div>
-              <div className="flex items-center justify-between p-3 bg-[var(--background)] rounded-lg">
+              <div className="flex items-center justify-between p-2 sm:p-3 bg-[var(--background)] rounded-lg">
                 <div className="flex items-center gap-2">
-                  <Clock className="h-4 w-4 text-[var(--muted)]" />
-                  <span className="text-sm">Running Requests</span>
+                  <Clock className="h-3 sm:h-4 w-3 sm:w-4 text-[var(--muted)]" />
+                  <span className="text-xs sm:text-sm">Running</span>
                 </div>
-                <span className="font-mono font-semibold">{metrics?.running_requests || 0}</span>
+                <span className="font-mono font-semibold text-sm">{metrics?.running_requests || 0}</span>
               </div>
-              <div className="flex items-center justify-between p-3 bg-[var(--background)] rounded-lg">
+              <div className="flex items-center justify-between p-2 sm:p-3 bg-[var(--background)] rounded-lg">
                 <div className="flex items-center gap-2">
-                  <Clock className="h-4 w-4 text-[var(--muted)]" />
-                  <span className="text-sm">Pending Requests</span>
+                  <Clock className="h-3 sm:h-4 w-3 sm:w-4 text-[var(--muted)]" />
+                  <span className="text-xs sm:text-sm">Pending</span>
                 </div>
-                <span className="font-mono font-semibold">{metrics?.pending_requests || 0}</span>
+                <span className="font-mono font-semibold text-sm">{metrics?.pending_requests || 0}</span>
               </div>
             </div>
           </section>
 
-          <section className="bg-[var(--card)] border border-[var(--border)] rounded-lg p-6">
-            <div className="flex items-center justify-between mb-3">
+          <section className="bg-[var(--card)] border border-[var(--border)] rounded-lg p-4 sm:p-6">
+            <div className="flex items-center justify-between mb-2 sm:mb-3">
               <h2 className="text-xs font-medium text-[var(--muted-foreground)] uppercase tracking-wide">Recipes ({recipes.length})</h2>
               <button
                 onClick={() => router.push('/recipes?new=1')}
@@ -404,7 +455,7 @@ export default function Dashboard() {
                 + New
               </button>
             </div>
-            <div className="space-y-1 max-h-80 overflow-y-auto pr-1">
+            <div className="space-y-1 max-h-60 sm:max-h-80 overflow-y-auto pr-1">
               {recipes.map((recipe) => (
                 <div
                   key={recipe.id}
@@ -436,6 +487,41 @@ export default function Dashboard() {
           </section>
         </div>
       </div>
+
+      {/* Launch progress toast */}
+      {launchProgress && (
+        <div className="fixed bottom-20 sm:bottom-6 left-4 right-4 sm:left-auto sm:right-6 z-50 px-3 sm:px-4 py-2 sm:py-3 bg-[var(--card)] border border-[var(--border)] rounded-lg shadow-xl sm:max-w-md animate-in slide-in-from-bottom-2 duration-300">
+          <div className="flex items-center gap-3">
+            {launchProgress.stage === 'error' ? (
+              <div className="w-8 h-8 rounded-full bg-red-500/20 flex items-center justify-center flex-shrink-0">
+                <X className="h-4 w-4 text-red-400" />
+              </div>
+            ) : launchProgress.stage === 'ready' ? (
+              <div className="w-8 h-8 rounded-full bg-green-500/20 flex items-center justify-center flex-shrink-0">
+                <Check className="h-4 w-4 text-green-400" />
+              </div>
+            ) : (
+              <div className="w-8 h-8 rounded-full bg-blue-500/20 flex items-center justify-center flex-shrink-0">
+                <Activity className="h-4 w-4 animate-spin text-blue-400" />
+              </div>
+            )}
+
+            <div className="flex-1 min-w-0">
+              <div className="text-sm font-medium capitalize">{launchProgress.stage}</div>
+              <div className="text-xs text-[var(--muted-foreground)] mt-0.5 truncate">{launchProgress.message}</div>
+
+              {launchProgress.progress !== undefined && launchProgress.stage !== 'ready' && launchProgress.stage !== 'error' && (
+                <div className="mt-2 h-1 bg-[var(--border)] rounded-full overflow-hidden">
+                  <div
+                    className="h-full bg-blue-500 transition-all duration-300"
+                    style={{ width: `${Math.round(launchProgress.progress * 100)}%` }}
+                  />
+                </div>
+              )}
+            </div>
+          </div>
+        </div>
+      )}
     </div>
   );
 }
