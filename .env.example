# vLLM Studio Configuration
# Copy this file to .env and modify as needed

# =============================================================================
# Controller Settings
# =============================================================================

# API server settings
VLLM_STUDIO_HOST=0.0.0.0
VLLM_STUDIO_PORT=8080

# Optional API authentication (leave empty for no auth)
VLLM_STUDIO_API_KEY=

# Inference backend settings (where vLLM/SGLang/llama.cpp runs)
# For external servers, set the host to the remote IP address
VLLM_STUDIO_INFERENCE_HOST=localhost
VLLM_STUDIO_INFERENCE_PORT=8000

# =============================================================================
# Paths
# =============================================================================

# Directory containing model weights
VLLM_STUDIO_MODELS_DIR=/models

# Data directory for recipes and chat history
VLLM_STUDIO_DATA_DIR=./data

# =============================================================================
# Backend-specific Settings (Optional)
# =============================================================================

# SGLang Python path (only needed if using SGLang backend)
# VLLM_STUDIO_SGLANG_PYTHON=/path/to/sglang/venv/bin/python

# TabbyAPI directory (only needed if using TabbyAPI/ExLlamaV3 backend)
# VLLM_STUDIO_TABBY_API_DIR=/path/to/tabbyAPI

# llama.cpp server binary path (only needed if using llama.cpp backend)
# Build llama.cpp: git clone https://github.com/ggml-org/llama.cpp && cd llama.cpp && make -j LLAMA_CUDA=1
# LLAMA_SERVER_PATH=/path/to/llama.cpp/build/bin/llama-server

# =============================================================================
# LiteLLM Gateway (Optional - for API routing/format translation)
# =============================================================================

LITELLM_MASTER_KEY=sk-master

# Backend inference URL (used by LiteLLM to route requests)
INFERENCE_API_BASE=http://localhost:8000/v1
INFERENCE_API_KEY=sk-placeholder

# =============================================================================
# Frontend Settings
# =============================================================================

# LiteLLM URL for chat (defaults to localhost if not set)
NEXT_PUBLIC_LITELLM_URL=http://localhost:4100

# =============================================================================
# Monitoring (Optional)
# =============================================================================

# Grafana admin password
GRAFANA_PASSWORD=admin

# =============================================================================
# Search Integration (Optional)
# =============================================================================

# Exa AI API key for web search (used in chat for research mode)
# Get your API key from https://exa.ai
EXA_API_KEY=your-exa-api-key-here
