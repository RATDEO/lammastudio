# LiteLLM Proxy Configuration
# Handles API routing, format translation, and cost tracking

model_list:
  # Local vLLM backend (managed by controller)
  - model_name: "*"
    litellm_params:
      model: "hosted_vllm/*"
      api_base: "http://host.docker.internal:8000/v1"
      stream_timeout: 600
      timeout: 600
      # Support for extended features
      supports_function_calling: true
      supports_vision: true
      supports_response_schema: true

# Router settings
router_settings:
  routing_strategy: "simple-shuffle"
  num_retries: 3
  timeout: 600
  retry_after: 3
  enable_pre_call_checks: true
  # Better connection handling
  cooldown_time: 0
  allowed_fails: 3

# LiteLLM settings
litellm_settings:
  drop_params: true
  set_verbose: false
  request_timeout: 600
  telemetry: false
  # Enhanced streaming and connection handling
  stream_chunk_size: 1024
  num_retries: 3
  max_budget: 0
  budget_duration: 0
  # Format handling
  modify_params: true
  # Tool calling support
  enable_message_redaction: false
  force_ipv4: true

# General settings
general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  # Newer LiteLLM images use Prisma with Postgres (env DATABASE_URL).
  database_url: "postgresql://postgres:postgres@vllm-studio-postgres:5432/litellm"

  # Disable UI in production, enable for debugging
  ui_access_mode: "admin_only"

  # Logging
  json_logs: true
  store_model_in_db: true

  # Health and monitoring
  alerting: null
  health_check_interval: 30
  disable_end_user_cost_tracking: false

  # Connection resilience
  disable_retry_on_max_parallel_request_limit_error: false
