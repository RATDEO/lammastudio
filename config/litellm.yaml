# LiteLLM Proxy Configuration
# Handles API routing, format translation, and cost tracking

model_list:
  # Wildcard catch-all for any model name -> route to local TabbyAPI/vLLM
  - model_name: "*"
    litellm_params:
      model: "openai/*"
      api_base: "http://172.18.0.1:8000/v1"
      api_key: "2203f577688173dad689c6f65884778c"
      stream_timeout: 600
      timeout: 600
      supports_function_calling: true
      supports_vision: true
      supports_response_schema: true

  # Explicit model for health checks (uses currently loaded model)
  - model_name: "glm-4.6v"
    litellm_params:
      model: "openai/glm-4.6v"
      api_base: "http://172.18.0.1:8000/v1"
      api_key: "2203f577688173dad689c6f65884778c"
      stream_timeout: 600
      timeout: 600
      supports_function_calling: true
      supports_vision: true

  # GLM-4.7 on ExLlamaV3/TabbyAPI (via tool call proxy)
  - model_name: "glm-4.7"
    litellm_params:
      model: "openai/glm-4.7"
      api_base: "http://172.18.0.1:8001/v1"
      api_key: "2203f577688173dad689c6f65884778c"
      stream_timeout: 600
      timeout: 600
      supports_function_calling: true
      supports_tool_choice: true
      # Try to preserve tool_calls from backend
      custom_llm_provider: openai

  # GLM-4.7 on ExLlamaV3/TabbyAPI (by exact model ID, via proxy)
  - model_name: "GLM-4.7-EXL3-3bpw_H6"
    litellm_params:
      model: "openai/glm-4.7"
      api_base: "http://172.18.0.1:8001/v1"
      api_key: "2203f577688173dad689c6f65884778c"
      stream_timeout: 600
      timeout: 600
      supports_function_calling: true
      custom_llm_provider: openai

# Router settings
router_settings:
  routing_strategy: "simple-shuffle"
  num_retries: 3
  timeout: 600
  retry_after: 3
  enable_pre_call_checks: false
  # Better connection handling
  cooldown_time: 0
  allowed_fails: 3

# LiteLLM settings
litellm_settings:
  drop_params: true
  set_verbose: false
  # Tool calls are handled by the proxy on port 8001
  request_timeout: 600
  telemetry: false
  # Enhanced streaming and connection handling
  stream_chunk_size: 1024
  num_retries: 3
  max_budget: 0
  budget_duration: 0
  # Format handling - keep false to preserve tool_calls from backend
  modify_params: false
  # Tool calling support
  enable_message_redaction: false
  force_ipv4: true

  # Redis caching for LLM responses
  cache: true
  cache_params:
    type: "redis"
    host: "vllm-studio-redis"
    port: 6379
    ttl: 3600  # 1 hour cache
    namespace: "litellm:cache"
    supported_call_types: ["acompletion", "completion", "embedding"]

  # Prometheus metrics collection
  success_callback: ["prometheus"]
  failure_callback: ["prometheus"]

# General settings
general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  # Newer LiteLLM images use Prisma with Postgres (env DATABASE_URL).
  database_url: "postgresql://postgres:postgres@vllm-studio-postgres:5432/litellm"

  # Disable UI in production, enable for debugging
  ui_access_mode: "admin_only"

  # Logging
  json_logs: true
  store_model_in_db: true

  # Health and monitoring
  health_check_interval: 30
  disable_end_user_cost_tracking: false

  # Alerting configuration (disabled - no Slack webhook configured)
  # alerting: ["slack"]
  # alerting_threshold: 300
  # slack_webhook_url: os.environ/SLACK_WEBHOOK_URL

  # Performance optimizations
  batch_spend_updates: true
  spend_update_interval: 60  # Update spend every 60s

  # Connection resilience
  disable_retry_on_max_parallel_request_limit_error: false
