# LiteLLM Proxy Configuration
# Handles API routing, format translation, and cost tracking

model_list:
  # Wildcard catch-all for any model name -> route to local inference server
  - model_name: "*"
    litellm_params:
      model: "openai/*"
      api_base: "${INFERENCE_API_BASE:-http://localhost:8000/v1}"
      api_key: "${INFERENCE_API_KEY:-sk-placeholder}"
      stream_timeout: 600
      timeout: 600
      supports_function_calling: true
      supports_vision: true
      supports_response_schema: true

# Router settings
router_settings:
  routing_strategy: "simple-shuffle"
  num_retries: 3
  timeout: 600
  retry_after: 3
  enable_pre_call_checks: false
  cooldown_time: 0
  allowed_fails: 3

# LiteLLM settings
litellm_settings:
  drop_params: true
  set_verbose: false
  request_timeout: 600
  telemetry: false
  stream_chunk_size: 1024
  num_retries: 3
  max_budget: 0
  budget_duration: 0
  modify_params: false
  enable_message_redaction: false
  force_ipv4: true

  # Redis caching for LLM responses (optional)
  cache: true
  cache_params:
    type: "redis"
    host: "vllm-studio-redis"
    port: 6379
    ttl: 3600
    namespace: "litellm:cache"
    supported_call_types: ["acompletion", "completion", "embedding"]

  # Prometheus metrics collection
  success_callback: ["prometheus"]
  failure_callback: ["prometheus"]

# General settings
general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  database_url: "postgresql://postgres:postgres@vllm-studio-postgres:5432/litellm"
  ui_access_mode: "admin_only"
  json_logs: true
  store_model_in_db: true
  health_check_interval: 30
