# Controlling MiniMax-M2 Thinking Tokens with vLLM

## Overview

MiniMax-M2 models use a special "thinking" phase where they generate reasoning in `` tags before producing the final answer. Without limits, this can lead to:
- Infinite thinking loops
- Excessive token usage
- Slow responses

This guide shows you how to control `max_thinking_tokens` using vLLM to prevent these issues while maintaining model performance.

---

## Quick Start

### Default Behavior (16K Token Limit)

```bash
vllm serve MiniMaxAI/MiniMax-M2.1 \
  --reasoning-parser minimax_m2_append_think \
  --default-chat-template-kwargs '{"max_thinking_tokens": 16384}'
```

**Result**: Model stops thinking after 16,384 tokens and proceeds to final answer.

---

## Understanding Thinking Tokens

### What Are Thinking Tokens?

MiniMax-M2 generates responses in two phases:

1. **Thinking Phase** (`` tags)
   - Internal reasoning and planning
   - Can be extensive for complex problems
   - **This is what we limit**

2. **Response Phase** (final answer)
   - The actual output shown to user
   - Not affected by `max_thinking_tokens`

### Example

```
User: "What's 2+2?"

Model Output:


The answer is 4.
```

With `max_thinking_tokens=100`, the thinking would be truncated much earlier.

---

## vLLM Configuration Options

### Option 1: Command Line (Simple)

#### Conservative (16K) - Recommended Default

```bash
vllm serve MiniMaxAI/MiniMax-M2.1 \
  --reasoning-parser minimax_m2_append_think \
  --default-chat-template-kwargs '{"max_thinking_tokens": 16384}'
```

**Use for**: Most tasks, chatbots, Q&A, fast responses

#### Balanced (64K) - Standard Depth

```bash
vllm serve MiniMaxAI/MiniMax-M2.1 \
  --reasoning-parser minimax_m2_append_think \
  --default-chat-template-kwargs '{"max_thinking_tokens": 65536}'
```

**Use for**: Coding tasks, standard reasoning, general use

#### Aggressive (128K) - Complex Tasks

```bash
vllm serve MiniMaxAI/MiniMax-M2.1 \
  --reasoning-parser minimax_m2_append_think \
  --default-chat-template-kwargs '{"max_thinking_tokens": 131072}'
```

**Use for**: Multi-step problems, architecture design, research

#### Disabled (No Thinking)

```bash
vllm serve MiniMaxAI/MiniMax-M2.1 \
  --reasoning-parser minimax_m2_append_think \
  --default-chat-template-kwargs '{"enable_thinking": false}'
```

**Use for**: Simple generation, fastest responses

---

### Option 2: Python API

```python
from vllm import LLM, SamplingParams

# Initialize with thinking token limit
llm = LLM(
    model="MiniMaxAI/MiniMax-M2.1",
    reasoning_parser="minimax_m2_append_think",
    default_chat_template_kwargs={"max_thinking_tokens": 16384}
)

# Generate response
prompts = ["What is the capital of France?"]
sampling_params = SamplingParams(temperature=0.7, max_tokens=100)
outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    print(output.outputs[0].text)
```

---

### Option 3: OpenAI-Compatible API

#### Start Server

```bash
vllm serve MiniMaxAI/MiniMax-M2.1 \
  --host 0.0.0.0 \
  --port 8000 \
  --reasoning-parser minimax_m2_append_think \
  --default-chat-template-kwargs '{"max_thinking_tokens": 16384}'
```

#### Client Usage

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="dummy"
)

response = client.chat.completions.create(
    model="MiniMaxAI/MiniMax-M2.1",
    messages=[
        {"role": "user", "content": "Explain quantum computing"}
    ]
)

# Access reasoning (if available)
if hasattr(response.choices[0].message, 'reasoning'):
    print("Reasoning:", response.choices[0].message.reasoning)

print("Answer:", response.choices[0].message.content)
```

---

## Token Limits by Model

| Model | Conservative | Balanced | Aggressive |
|-------|--------------|----------|------------|
| **MiniMax-M2.1** | 16,384 | 65,536 | 131,072 |
| **MiniMax-M2.1-REAP-50** | 32,768 | 131,072 | 262,144 |
| **GLM-4.7** | 8,192 | 32,768 | 65,536 |
| **DeepSeek-R1** | 16,384 | 65,536 | 131,072 |

**Note**: Values are in tokens. Multiply by ~4 for approximate character count.

---

## How to Choose the Right Limit

### Conservative (16K) - Use When:

âœ… Simple questions and Q&A
âœ… Chatbots and customer service
âœ… Real-time interactions
âœ… Cost-sensitive applications
âœ… **Default choice for most use cases**

**Examples**:
```
User: "What's the weather like?"
User: "How do I reset my password?"
User: "Translate this to Spanish"
```

### Balanced (64K) - Use When:

âœ… Standard coding tasks
âœ… Multi-step reasoning
âœ… General-purpose assistant
âœ… When you need moderate depth

**Examples**:
```
User: "Write a Python function to parse JSON"
User: "Explain how neural networks work"
User: "Debug this code"
```

### Aggressive (128K) - Use When:

âœ… Complex architectural design
âœ… Research and analysis
âœ… Multi-file code generation
âœ… When you need maximum depth

**Examples**:
```
User: "Design a distributed system architecture"
User: "Analyze this 10,000-line codebase"
User: "Create a comprehensive business strategy"
```

---

## What Happens When Limit is Reached?

### Behavior

1. **Thinking phase stops** - `` tags close
2. **Model transitions** to response generation
3. **Final answer produced** - Based on reasoning so far

### Example with Low Limit

```
max_thinking_tokens = 100

User: "Design a microservices architecture"

Model Output:
<think>
I need to design a microservices architecture.
Key components include:
- API Gateway
- Service discovery
- Load balancing
[thinking truncated at 100 tokens]
</think>

I'll design a microservices architecture with these key components...

[Rest of answer based on partial reasoning]
```

**Result**: Answer is less detailed but still functional.

**Solution**: Increase to `max_thinking_tokens=131072` for full depth.

---

## Advanced Configuration

### Multi-GPU with Tensor Parallelism

```bash
vllm serve MiniMaxAI/MiniMax-M2.1 \
  --tensor-parallel-size 4 \
  --reasoning-parser minimax_m2_append_think \
  --default-chat-template-kwargs '{"max_thinking_tokens": 16384}'
```

### REAP-50 Variant (Extended Context)

```bash
vllm serve MiniMaxAI/MiniMax-M2.1-REAP-50 \
  --tensor-parallel-size 4 \
  --enable-expert-parallel \
  --reasoning-parser minimax_m2_append_think \
  --default-chat-template-kwargs '{"max_thinking_tokens": 32768}'
```

### Custom Value (Fine-Tuned)

```bash
vllm serve MiniMaxAI/MiniMax-M2.1 \
  --reasoning-parser minimax_m2_append_think \
  --default-chat-template-kwargs '{"max_thinking_tokens": 42000}'
```

---

## Performance Comparison

### Response Time by Token Limit

| Limit | Simple Query | Complex Task | Token Usage |
|-------|--------------|--------------|-------------|
| **16K** | ~2 sec | ~10 sec | 75% less |
| **64K** | ~5 sec | ~30 sec | Baseline |
| **128K** | ~10 sec | ~60 sec | 2x more |

### Quality Impact

Research shows:
- **16K sufficient for 80%+ of tasks**
- **Quality loss only on very complex problems**
- **Forced efficiency often improves answers**

---

## Troubleshooting

### Problem: Model Stops Mid-Thought

**Symptom**: `` tags close, reasoning seems incomplete

**Solution**:
```bash
# Increase limit
--default-chat-template-kwargs '{"max_thinking_tokens": 65536}'
```

### Problem: Responses Too Slow

**Symptom**: Long wait times before answer

**Solution**:
```bash
# Decrease limit
--default-chat-template-kwargs '{"max_thinking_tokens": 8192}'
```

### Problem: Not Enough Detail

**Symptom**: Answers lack depth

**Solution**:
```bash
# Switch to aggressive mode
--default-chat-template-kwargs '{"max_thinking_tokens": 131072}'
```

---

## Best Practices

### 1. Start Conservative

Begin with 16K tokens. Only increase if needed.

```bash
--default-chat-template-kwargs '{"max_thinking_tokens": 16384}'
```

### 2. Monitor Token Usage

Check logs for actual token consumption:

```bash
# vLLM logs show token usage
tail -f /var/log/vllm.log | grep "thinking_tokens"
```

### 3. Adjust Based on Task Type

- **Production**: Conservative (16K)
- **Development**: Balanced (64K)
- **Research**: Aggressive (128K)

### 4. Test Your Workload

Run sample queries with different limits:

```python
test_queries = [
    ("Simple", "What's 2+2?"),
    ("Medium", "Write a sorting algorithm"),
    ("Complex", "Design a payment system")
]

for name, query in test_queries:
    # Test with different limits
    for limit in [16384, 65536, 131072]:
        # Measure quality, time, and cost
        pass
```

---

## Complete Example: Production Deployment

### Docker Compose

```yaml
version: '3.8'
services:
  vllm-minimax:
    image: vllm/vllm-openai:latest
    command: >
      --model MiniMaxAI/MiniMax-M2.1
      --host 0.0.0.0
      --port 8000
      --tensor-parallel-size 4
      --gpu-memory-utilization 0.9
      --reasoning-parser minimax_m2_append_think
      --default-chat-template-kwargs '{"max_thinking_tokens": 16384}'
    ports:
      - "8000:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 4
              capabilities: [gpu]
```

### Python Service Wrapper

```python
from fastapi import FastAPI
from openai import OpenAI

app = FastAPI()
client = OpenAI(base_url="http://vllm-minimax:8000/v1", api_key="dummy")

@app.post("/chat")
async def chat(message: str):
    response = client.chat.completions.create(
        model="MiniMaxAI/MiniMax-M2.1",
        messages=[{"role": "user", "content": message}],
        max_tokens=512  # Response limit (not thinking limit)
    )
    return {
        "reasoning": response.choices[0].message.reasoning,
        "answer": response.choices[0].message.content
    }
```

---

## Key Takeaways

âœ… **Start with 16K tokens** - Works for 80%+ of tasks  
âœ… **Models adapt to budget** - Don't "request more"  
âœ… **Easy to adjust** - Single parameter change  
âœ… **Monitor and tune** - Find what works for your workload  
âœ… **Trade-offs exist** - Speed vs depth, cost vs quality  

---

## References

- [vLLM Documentation](https://docs.vllm.ai/en/latest/)
- [MiniMax-M2 Deployment Guide](https://huggingface.co/MiniMaxAI/MiniMax-M2.1)
- [Reasoning Outputs Feature](https://docs.vllm.ai/en/latest/features/reasoning_outputs/)
- [Interleaved Thinking](https://docs.vllm.ai/en/latest/features/interleaved_thinking/)

---

## Summary Table

| Scenario | Recommended Limit | Command |
|----------|-------------------|---------|
| **Most Production** | 16K | `--default-chat-template-kwargs '{"max_thinking_tokens": 16384}'` |
| **General Purpose** | 64K | `--default-chat-template-kwargs '{"max_thinking_tokens": 65536}'` |
| **Complex Tasks** | 128K | `--default-chat-template-kwargs '{"max_thinking_tokens": 131072}'` |
| **No Thinking** | Disabled | `--default-chat-template-kwargs '{"enable_thinking": false}'` |
| **Custom** | Any | `--default-chat-template-kwargs '{"max_thinking_tokens": YOUR_VALUE}'` |

---

**Happy optimizing! ðŸš€**

Remember: Lower limits = faster, cheaper responses. Start conservative and scale up as needed.
